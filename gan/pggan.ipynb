{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82de40dc",
   "metadata": {
    "id": "fff681d6-2c35-4198-9141-a98c67db7285"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from math import log2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b207fe",
   "metadata": {
    "executionInfo": {
     "elapsed": 4165,
     "status": "ok",
     "timestamp": 1618916347704,
     "user": {
      "displayName": "Nazwa Kurde",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi6wRf_U46CDUN6sLVZnOElTALo2wVxRbNSH3pXYQ=s64",
      "userId": "15397366994888973337"
     },
     "user_tz": -120
    },
    "id": "lP6mu7xVEHam"
   },
   "outputs": [],
   "source": [
    "def plot_to_tensorboard(\n",
    "    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n",
    "):\n",
    "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # take out (up to) 8 examples to plot\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b05e5a2",
   "metadata": {
    "id": "df946099-8e1f-462c-9563-e7e752860284"
   },
   "outputs": [],
   "source": [
    "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbf72b9",
   "metadata": {
    "id": "0638027c-4560-4479-9fa9-4513a7264f00"
   },
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels,out_channels,\n",
    "        kernel_size=3,stride=1,padding=1,gain=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n",
    "        self.scale = (gain/(in_channels*kernel_size**2))**.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "        \n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "    \n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.eps)\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,use_pixelnorm=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels,out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels,out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pixelnorm = use_pixelnorm\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pixelnorm else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pixelnorm else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edabfbef",
   "metadata": {
    "id": "227ee556-eda8-40c6-9345-6f145ce9edd7"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_dim,in_channels,img_channels=3):\n",
    "        super().__init__()\n",
    "        self.first = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, \n",
    "            stride=1, padding=0\n",
    "        )\n",
    "        \n",
    "        self.prog_blocks = nn.ModuleList([])\n",
    "        self.rgb_layers = nn.ModuleList([self.initial_rgb])\n",
    "        \n",
    "        for i in range(len(factors)-1):\n",
    "            conv_in_channels = int(in_channels*factors[i])\n",
    "            conv_out_channels = int(in_channels*factors[i+1])\n",
    "            \n",
    "            conv_in_channels = int(in_channels * factors[i])\n",
    "            conv_out_channels = int(in_channels * factors[i + 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_channels, conv_out_channels))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_channels, img_channels, kernel_size=1, stride=1, padding=0))\n",
    "    \n",
    "    def fade_in(self,alpha,upscaled,generated):\n",
    "        return torch.tanh(alpha*generated +(1-alpha)*upscaled)\n",
    "        \n",
    "    def forward(self,x,alpha,steps):\n",
    "        out = self.first(x) # 4x4\n",
    "        if steps==0:\n",
    "            return self.initial_rgb(out)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "            \n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ede7f1",
   "metadata": {
    "id": "30962e46-d094-474b-9596-acf5f59d58ac"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.prog_blocks = nn.ModuleList([])\n",
    "        self.rgb_layers = nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(.2)\n",
    "        \n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0))\n",
    "            \n",
    "        \n",
    "        # only for 4x4 resolution\n",
    "        self.end_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.rgb_layers.append(self.end_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # block for 4x4 resolution\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, kernel_size=1, padding=0, stride=1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def fade_in(self,alpha,downscaled,out):\n",
    "        return alpha*out+(1-alpha)*downscaled\n",
    "        \n",
    "    def minibatch_std(self,x):\n",
    "        batch_statistics = torch.std(x,dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x,batch_statistics],dim=1)\n",
    "    \n",
    "    def forward(self,x,alpha,steps): \n",
    "        current_step = len(self.prog_blocks) - steps\n",
    "        out = self.leaky(self.rgb_layers[current_step](x))\n",
    "        \n",
    "        if steps == 0:  # 4x4\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "        \n",
    "        downscaled = self.leaky(self.rgb_layers[current_step+1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[current_step](out))\n",
    "        out = self.fade_in(alpha,downscaled,out)\n",
    "        for step in range(current_step+1,len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "            \n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5ef9c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9754,
     "status": "ok",
     "timestamp": 1618904644991,
     "user": {
      "displayName": "Nazwa Kurde",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi6wRf_U46CDUN6sLVZnOElTALo2wVxRbNSH3pXYQ=s64",
      "userId": "15397366994888973337"
     },
     "user_tz": -120
    },
    "id": "d60f533e-c3ee-441f-934c-f3beeb12e7a0",
    "outputId": "2fb274a8-65d8-4305-91ac-af6352de8163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 4x4 OK: 8x8 OK: 16x16 OK: 32x32 OK: 64x64 OK: 128x128 OK: 256x256 OK: 512x512 OK: 1024x1024 "
     ]
    }
   ],
   "source": [
    "# test\n",
    "Z_DIM = 100\n",
    "IN_CHANNELS = 256\n",
    "gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "critic = Critic(Z_DIM, IN_CHANNELS, img_channels=3)\n",
    "\n",
    "for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "    num_steps = int(log2(img_size/4))\n",
    "    x =  torch.randn((1,Z_DIM,1,1))\n",
    "    z = gen(x,.5,steps=num_steps)\n",
    "    assert z.shape==(1,3,img_size,img_size)\n",
    "    out = critic(z,alpha=.5,steps=num_steps)\n",
    "    assert out.shape == (1,1)\n",
    "    print(f\"OK: {img_size}x{img_size}\",end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61877bff",
   "metadata": {
    "id": "cd797b99-7020-420f-b247-e7a589d1d07b"
   },
   "outputs": [],
   "source": [
    "START_TRAIN_AT_IMG_SIZE = 256\n",
    "DATASET = '../content/drive/MyDrive/'\n",
    "CHECKPOINT_GEN = \"generator.pth\"\n",
    "CHECKPOINT_CRITIC = \"critic.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = True\n",
    "LR = 1e-3\n",
    "BATCH_SIZES = [2048, 1024, 512, 128, 32, 16, 10, 4, 2]\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 512  # 256/512\n",
    "IN_CHANNELS = 512  # 256/512\n",
    "CRITIC_ITERATIONS = 1\n",
    "LAMBDA_GP = 10\n",
    "PROGRESSIVE_EPOCHS = [64] * len(BATCH_SIZES) # more (?)\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "torch.backends.cudnn.benchmarks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db2e4d6",
   "metadata": {
    "id": "748d2add-87e7-4f56-a463-2b37073d5d6d"
   },
   "outputs": [],
   "source": [
    "def generate_examples(gen, steps, truncation=0.7, n=100):\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, Z_DIM, 1, 1)), device=DEVICE, dtype=torch.float32)\n",
    "            img = gen(noise, alpha, steps)\n",
    "            save_image(img*0.5+0.5, f\"saved/img_{i}.png\")\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc03881",
   "metadata": {
    "id": "6c1ea9c8-12e6-4218-96de-f12a18c0ece7"
   },
   "outputs": [],
   "source": [
    "def get_loader(image_size):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "                [0.5 for _ in range(CHANNELS_IMG)],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n",
    "    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0189d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# l,d = get_loader(4 * 2 ** step)\n",
    "# a = iter(d)\n",
    "# next(a)\n",
    "# next(a)\n",
    "# plt.imshow(next(a)[0].permute(1,2,0)*0.5+0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30348eb7",
   "metadata": {
    "id": "5a824885-96bb-4f5b-8588-8861798e3906"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    loader,\n",
    "    dataset,\n",
    "    step,\n",
    "    alpha,\n",
    "    opt_critic,\n",
    "    opt_gen,\n",
    "    tensorboard_step,\n",
    "    writer,\n",
    "    scaler_gen,\n",
    "    scaler_critic,\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise, alpha, step)\n",
    "            critic_real = critic(real, alpha, step)\n",
    "            critic_fake = critic(fake.detach(), alpha, step)\n",
    "            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                + LAMBDA_GP * gp\n",
    "                + (0.001 * torch.mean(critic_real ** 2))\n",
    "            )\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        alpha += cur_batch_size / (\n",
    "            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 256 == 0 and batch_idx:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                loss_critic.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tensorboard_step,\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n",
    "\n",
    "    return tensorboard_step, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41108d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n",
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n",
    "critic = Critic(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n",
    "\n",
    "# initialize optimizers and scalers for FP16 training\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LR, betas=(0.0, 0.99))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LR, betas=(0.0, 0.99))\n",
    "scaler_critic = torch.cuda.amp.GradScaler()\n",
    "scaler_gen = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# for tensorboard plotting\n",
    "writer = SummaryWriter(f\"logs\")\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(\n",
    "        CHECKPOINT_GEN, gen, opt_gen, LR,\n",
    "    )\n",
    "    load_checkpoint(\n",
    "        CHECKPOINT_CRITIC, critic, opt_critic, LR,\n",
    "    )\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "tensorboard_step = 0\n",
    "step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9a95c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "executionInfo": {
     "elapsed": 89453,
     "status": "error",
     "timestamp": 1618905723267,
     "user": {
      "displayName": "Nazwa Kurde",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi6wRf_U46CDUN6sLVZnOElTALo2wVxRbNSH3pXYQ=s64",
      "userId": "15397366994888973337"
     },
     "user_tz": -120
    },
    "id": "85adcc21-2041-4ef8-9a15-9b68a5e7e719",
    "outputId": "5ce0e2d6-3fc7-47bb-ab0a-5ca1df58cc20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 256, Step: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [24:03<00:00,  2.89s/it, gp=0.0673, loss_critic=1.42]  \n",
      "100%|██████████| 500/500 [24:05<00:00,  2.89s/it, gp=0.0835, loss_critic=-2.6]  \n",
      "100%|██████████| 500/500 [24:07<00:00,  2.89s/it, gp=0.111, loss_critic=-.15]   \n",
      "100%|██████████| 500/500 [24:08<00:00,  2.90s/it, gp=0.0281, loss_critic=-6.37]  \n",
      "100%|██████████| 500/500 [24:08<00:00,  2.90s/it, gp=0.155, loss_critic=-10.5]   \n",
      "100%|██████████| 500/500 [24:09<00:00,  2.90s/it, gp=0.0106, loss_critic=-3.35] \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0122, loss_critic=-1.19] \n",
      "100%|██████████| 500/500 [24:09<00:00,  2.90s/it, gp=0.134, loss_critic=5.54]    \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0196, loss_critic=1.69]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0444, loss_critic=-1.15]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.00818, loss_critic=-9.45] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0534, loss_critic=-7.1]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0643, loss_critic=-6.83]  \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0797, loss_critic=-12.3] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0966, loss_critic=-7.9]  \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0791, loss_critic=-4.31]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.016, loss_critic=1.59]   \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0561, loss_critic=2.76]   \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.16, loss_critic=-.164]    \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.016, loss_critic=-5.69]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0253, loss_critic=-6.59]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0185, loss_critic=-1.21] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0226, loss_critic=-3.1]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0215, loss_critic=-1.53]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0197, loss_critic=-3.26]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0701, loss_critic=-8.22]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0124, loss_critic=-8.34]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.00694, loss_critic=-5.44] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0442, loss_critic=-6.16] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0579, loss_critic=-9.62] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0335, loss_critic=-1.94]  \n",
      "100%|██████████| 500/500 [24:12<00:00,  2.90s/it, gp=0.0551, loss_critic=-.102] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0235, loss_critic=-12.6]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0956, loss_critic=-6.3]   \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0353, loss_critic=-1.1]  \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0764, loss_critic=-9]     \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0092, loss_critic=-1.96]  \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.014, loss_critic=-2.25]  \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0118, loss_critic=-.695]   \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0554, loss_critic=-9.67] \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0554, loss_critic=3.01]  \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0879, loss_critic=-.302] \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0441, loss_critic=-2.47]  \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0388, loss_critic=-5.6]  \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0839, loss_critic=-.0633] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0407, loss_critic=-1.01] \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.0369, loss_critic=-10.5] \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.00749, loss_critic=1.38] \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0237, loss_critic=2.35]   \n",
      "100%|██████████| 500/500 [24:11<00:00,  2.90s/it, gp=0.0388, loss_critic=0.801] \n",
      "100%|██████████| 500/500 [24:10<00:00,  2.90s/it, gp=0.00621, loss_critic=-2.28]\n",
      " 81%|████████▏ | 407/500 [19:41<04:29,  2.90s/it, gp=0.055, loss_critic=-10.6]  "
     ]
    }
   ],
   "source": [
    "for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "    alpha = 1e-5  # start with very low alpha\n",
    "    loader, dataset = get_loader(4 * 2 ** step)\n",
    "    print(f\"Image size: {4 * 2 ** step}, Step: {step}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tensorboard_step, alpha = train_fn(\n",
    "            critic,\n",
    "            gen,\n",
    "            loader,\n",
    "            dataset,\n",
    "            step,\n",
    "            alpha,\n",
    "            opt_critic,\n",
    "            opt_gen,\n",
    "            tensorboard_step,\n",
    "            writer,\n",
    "            scaler_gen,\n",
    "            scaler_critic,\n",
    "        )\n",
    "        generate_examples(gen,step,n=4)\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "            save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n",
    "\n",
    "    clear_output()\n",
    "    step += 1 # next img size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4289cc",
   "metadata": {
    "id": "d2acedc7-17b5-425d-bb1e-dd7bc360bf23"
   },
   "source": [
    "## Links:\n",
    "* https://youtu.be/nkQHASviYac\n",
    "* https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pggan.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
